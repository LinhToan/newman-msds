{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7V5f5qY2g2Y",
        "tags": []
      },
      "source": [
        "# Regression Application — STARTER NOTEBOOK\n",
        "\n",
        "**Applied Machine Learning 2 @ Newman University**\n",
        "\n",
        "*Prof. Ricky Boyer*\n",
        "\n",
        "**Linh Toan**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "oVVKOvV72g2a",
        "tags": []
      },
      "source": [
        "**Important note!** Before you turn in this lab notebook, make sure everything runs as expected:\n",
        "\n",
        "- First, **restart the kernel** -- in the menubar, select Kernel$\\rightarrow$Restart.\n",
        "- Then **run all cells** -- in the menubar, select Cell$\\rightarrow$Run All.\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "jEOIfbUN2g2b",
        "tags": []
      },
      "source": [
        "# Part 0: Sample dataset (Wine)\n",
        "\n",
        "The dataset is related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
        "\n",
        "In case you need to bring the data in yourself, this would be the location to find the raw data and description, though the following locked cells should bring it in for you.\n",
        "\n",
        "* Raw data and file description: http://https://archive.ics.uci.edu/dataset/186/wine+quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "LWMYudFP2g2c",
        "locked": false,
        "outputId": "98dc36e9-8637-42ae-d6c4-d982d39a6403",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "file = 'https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/winequality-combined.csv'\n",
        "wine_quality = pd.read_csv(file)\n",
        "\n",
        "print(\"\\n(All data appears to be ready.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "Pw99kPIH2g2e",
        "tags": []
      },
      "source": [
        "Let's take a look at the data, first as a table, taking interest in a few columns.  Then we can look at them using a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "editable": false,
        "id": "iXsCyAGl2g2f",
        "outputId": "149a5d69-1c88-4254-dffb-d4430765e791",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = wine_quality\n",
        "print(df.dtypes)\n",
        "display(df.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "editable": false,
        "id": "TmDsKnkW2g2g",
        "outputId": "c770a95a-329d-4a9e-9b46-dfd653f9d78a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import scatter, xlabel, title, plot\n",
        "%matplotlib inline\n",
        "\n",
        "scatter(df['volatile acidity'], df['pH'])\n",
        "xlabel ('Volatile Acidity')\n",
        "title ('Shocking news: Volatility is volatile!');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "ZQh5knNV2g2h",
        "tags": []
      },
      "source": [
        "## Fitting a model\n",
        "\n",
        "**Exercise 0** (40 points). Complete the function below so that it computes $\\alpha$ and $\\beta$ for the univariate model, $y \\sim \\alpha \\cdot x + \\beta$, given observations stored as NumPy arrays `y[:]` for the responses and `x[:]` for the predictor.\n",
        "\n",
        "If you need help solving this one, please see the [Additional Notebook](https://newmanu.instructure.com/courses/10664/files/1083442?wrap=1) resource in Module 1, as it has an in depth explanation of the math."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCrg2KDd2g2h",
        "outputId": "b2f103b9-55ba-4f7b-8ca6-acd56a8aff5e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def linreg_fit(x, y):\n",
        "    \"\"\"Returns (alpha, beta) s.t. y ~ alpha*x + beta.\"\"\"\n",
        "    from numpy import ones\n",
        "    m = len(x) ; assert len(y) == m\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    u = ones(m)\n",
        "    alpha = x.dot(y) - u.dot(x) * u.dot(y) / m\n",
        "    alpha /= x.dot(x) - (u.dot(x) ** 2) / m\n",
        "    beta = u.dot(y - alpha * x) / m\n",
        "    return (alpha, beta)\n",
        "\n",
        "# Compute the coefficients for the LSD data:\n",
        "x, y = df['volatile acidity'], df['pH']\n",
        "alpha, beta = linreg_fit(x, y)\n",
        "\n",
        "print(\"alpha:\", alpha)\n",
        "print(\"beta:\", beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "C9jPsySD2g2j",
        "outputId": "efe06af2-546b-410a-dd99-e470bb73c4c1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test cell: `linreg_fit_test`\n",
        "\n",
        "x, y = df['volatile acidity'], df['pH']\n",
        "alpha, beta = linreg_fit(x, y)\n",
        "\n",
        "r = alpha*x + beta - y\n",
        "ssqr = r.dot(r)\n",
        "ssqr_ex = 156.45802459454563\n",
        "\n",
        "from numpy import isclose\n",
        "assert isclose(ssqr, ssqr_ex, rtol=.01), \"Sum-of-squared residuals is {} instead of {}.\".format(ssqr, ssqr_ex)\n",
        "\n",
        "print(\"\\n(Passed! You got 40 points!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "editable": false,
        "id": "-YLBoBv32g2k",
        "outputId": "7db63440-03d1-4981-a307-9ed80d0aa6c6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from numpy import linspace, floor, ceil\n",
        "\n",
        "# Two points make a line:\n",
        "x_fit = linspace(floor(x.min()), ceil(x.max()), 2)\n",
        "y_fit = alpha*x_fit + beta\n",
        "\n",
        "scatter(x, y, marker='o')\n",
        "plot(x_fit, y_fit, 'r--')\n",
        "xlabel('Volatile Acidity')\n",
        "title('Best-fit linear model');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "kRYjR2WX2g2l",
        "tags": []
      },
      "source": [
        "**Congrats!** It appears the two are related afterall.\n",
        "If you've gotten this far without errors, you're ready to move to the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "bl4Nnp-m2g2l",
        "tags": []
      },
      "source": [
        "# Part 1: Simple Regression Application and Diagnostics (Wine)\n",
        "Now that we know about the background, let's try it using our [SciKit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) methodologies and see where they get us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": false,
        "id": "7MAsafxf2g2m",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell #\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.formula.api as smf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "1j1QuECj2g2m",
        "tags": []
      },
      "source": [
        "## Fitting a model\n",
        "\n",
        "**Exercise 1** (20 points). We know that for application purposes, there is a specific configuration array for implementing Linear Regression with SciKit. As such, use the .reshape (-1, 1) method on our 2 columns (in the form of numpy arrays), assigning X to the independent variable (volatile acidity) and y to the dependent (pH).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DekRwGEH2g2m",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## YOUR ANSWER HERE\n",
        "#BEGIN SOLUTION\n",
        "X = np.asarray(x)\n",
        "X = X.reshape(-1,1)\n",
        "y = np.asarray(y)\n",
        "y = y.reshape(-1,1)\n",
        "#END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "Bdb-1F7P2g2n",
        "outputId": "fde34dc3-e394-4705-f943-b1d1438f5568",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test cell: `linreg_fit_test`\n",
        "\n",
        "assert X.shape == (6497,1), \"Shape of Volatile Acidity array is not correct for linear regression\"\n",
        "assert y.shape == (6497,1), \"Shape of Volatile Acidity array is not correct for linear regression\"\n",
        "\n",
        "print(\"\\n(Passed! You got 20 points!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "2ZMpanTe2g2n",
        "tags": []
      },
      "source": [
        "**Exercise 2** (40 points). Based on our X and y arrays, we should be able to fit a model. Do so using the LinearRegression().fit method. Set alpha (float) equal to your linear regression .coef_ and beta equal to your model . intercept_ per our formula: $y \\sim \\alpha \\cdot x + \\beta$.\n",
        "\n",
        "\n",
        "\n",
        "Normally here we would also assign a train and test split, for now we'll ignore that to ensure that we all end up with the same answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvO2i4La2g2o",
        "outputId": "70e01f2c-28e0-4f61-9c56-2323fc3e0f7c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def simp_reg (X, y):\n",
        "    ## YOUR ANSWER HERE\n",
        "    #BEGIN SOLUTION\n",
        "    model = LinearRegression().fit(X,y)\n",
        "    alpha = model.coef_\n",
        "    beta = model.intercept_\n",
        "    #END SOLUTION\n",
        "    return (alpha, beta)\n",
        "simp_reg(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "cHSc9YYf2g2o",
        "outputId": "2fe9b269-c13b-4320-ff86-072e56a94bc5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test cell: `simp_reg_test`\n",
        "\n",
        "x, y = X, y\n",
        "alpha, beta = simp_reg(x, y)\n",
        "model = LinearRegression().fit(X, y)\n",
        "a1 = 0.2553414857091353\n",
        "b1 = 3.1317700255382817\n",
        "\n",
        "from numpy import isclose\n",
        "assert isclose(alpha, a1, rtol=.01), \"Alpha is {} instead of {}.\".format(alpha, a1)\n",
        "assert isclose(beta, b1, rtol=.01), \"Alpha is {} instead of {}.\".format(beta, b1)\n",
        "\n",
        "print(\"\\n(Passed! You got 40 points!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3NtlI222g2o"
      },
      "source": [
        "We should now be able to check our work from before and see how close the two look to one another!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "editable": false,
        "id": "sDxwwNwO2g2o",
        "outputId": "8015e9da-91a5-454f-f594-61a7cfcac44a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"alpha:\", alpha)\n",
        "print(\"beta:\", beta)\n",
        "y_pred = model.predict(X)\n",
        "scatter(X, y, marker='o')\n",
        "plot(x_fit, y_fit, 'r--')\n",
        "xlabel('Volatile Acidity')\n",
        "title('Deja Vu!');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2U230Wo2g2o"
      },
      "source": [
        "I wonder if this model meets our assumptions about linear regression. Let's check out the diagnostics of the model to see how things are shaped up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV1XensA2g2p",
        "tags": []
      },
      "source": [
        "**Exercise 3** (40 points). Import metrics from [Sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) and generate the following metrics with the variable names in parentheses:\n",
        "\n",
        "* Mean Absolute Error (`mae`)\n",
        "* Mean Squared Error (`mse`)\n",
        "* Root Mean Squared Error (`rmse`)\n",
        "* $r^{2}$ (`r2`)\n",
        "\n",
        "\n",
        "\n",
        "Normally here we would also assign a train and test split, for now we'll ignore that to ensure that we all end up with the same answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3E81TPW2g2p",
        "outputId": "c49767be-28d8-4b4b-c0c9-d70b392cabc7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "##YOUR ANSWER HERE\n",
        "#BEGIN SOLUTION\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y, y_pred)\n",
        "#END SOLUTION\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"r2:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "WSQllK6H2g2r",
        "outputId": "a2e2b818-ba5b-4f51-ec33-d21037fcc8c7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Test cell: `simp_reg_metrics`\n",
        "url = 'https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/1_2_3_sol.txt'\n",
        "sol = pd.read_csv(url, sep=\":\", header=None)\n",
        "sol = sol.transpose()\n",
        "headers = sol.iloc[0].values\n",
        "sol.columns = headers\n",
        "sol.drop(index=0, axis=0, inplace=True)\n",
        "maesol = sol['MAE'].sum()\n",
        "msesol = sol['MSE'].sum()\n",
        "rmsesol = sol['RMSE'].sum()\n",
        "r2sol = sol['r2'].sum()\n",
        "assert isclose(mae, maesol, rtol=.01), \"Alpha is {} instead of {}.\".format(mae, maesol)\n",
        "assert isclose(mse, msesol, rtol=.01), \"Alpha is {} instead of {}.\".format(mse, msesol)\n",
        "assert isclose(rmse, rmsesol, rtol=.01), \"Alpha is {} instead of {}.\".format(rmse, rmsesol)\n",
        "assert isclose(r2, r2sol, rtol=.01), \"Alpha is {} instead of {}.\".format(r2, r2sol)\n",
        "print(\"\\n(Passed! You got 40 points!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "vzCG8tv32g2r",
        "tags": []
      },
      "source": [
        "While these numbers do not mean much on our own, they can help us put things into context. Had they been multiple whole numbers, compared to the pH scale (which only goes to 14) that would tell us something quite different.\n",
        "\n",
        "![image.](attachment:ddb31bcd-ed95-454f-be72-5b5225a2fe4a.)\n",
        "\n",
        "The one that does give us pretty good insight into the value of the model is the $r^{2}$. This seems to indicated that only 6.8% of the randomness within the data is explained by the model's pattern recognition, indicating that overall it is not a great model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "UNiT2hvj2g2r",
        "tags": []
      },
      "source": [
        "## Checking for assumptions\n",
        "Let's see if our data violates any of the assumptions of linear regression. Namely, we'll be looking to check for:\n",
        "* Presumed Linear Relationships\n",
        "* [Heteroscedasticity vs Homoscedasticity](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/homoscedasticity/)\n",
        "* [Multivariate Residual Normality](https://www.geeksforgeeks.org/quantile-quantile-plots/)\n",
        "* Lack of Multicollinearity\n",
        "\n",
        "As we are performing simple linear regression presently, we can presume first that there is some inclination of a presumed linear relationship. Especially believing that the two acidity measures are related seems reasonable. There may be some other (non-linear) relationship, but for now we can brush past that one.\n",
        "\n",
        "Lack of multicollinearity, at this stage of our flow is also guaranteed to be true. The reasoning for that is: you can't have related variables when you are only testing against one! It is possible for the target variable and the independent variables to have a relationship with one anbother that would violate this assumption, but for now you'll just have to take my word for it. As such let's test for the two remaining assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "editable": false,
        "id": "_4QQK1mA2g2s",
        "outputId": "0c89b8f0-b8b9-4c62-f1f2-36c89759619f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "#Start by making combined dataframe of X and y\n",
        "data = df[[\"volatile acidity\",\"pH\"]]\n",
        "data.loc[:,\"predictions\"] = y_pred\n",
        "display(data.head(5))\n",
        "\n",
        "# Plot residuals\n",
        "sns.residplot(x=data[\"predictions\"], y=data[\"pH\"], data=data,lowess=True, line_kws={'color': 'red', 'lw': 1, 'alpha': 1})\n",
        "plt.ylabel(\"\")\n",
        "plt.xlabel(\"Fitted value\")\n",
        "\n",
        "# Q-Q Plot\n",
        "residuals = data[\"pH\"] - y_pred.reshape(-1)\n",
        "residuals\n",
        "plt.figure(figsize=(7,7))\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q Plot: Not as Normal as could be\")\n",
        "\n",
        "# Standardized Residuals\n",
        "model_norm_residuals_abs_sqrt=np.sqrt(np.abs(residuals))\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.regplot(x=data[\"predictions\"], y=model_norm_residuals_abs_sqrt, scatter=True, lowess=True, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
        "plt.ylabel(\"Standarized residuals\")\n",
        "plt.xlabel(\"Fitted value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUgRXGPN2g2s",
        "tags": []
      },
      "source": [
        "**Congrats!** We've now mastered simple regression by hand and using SciKit.\n",
        "If you've gotten this far without errors, you're ready to move to the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "09lHs6wy2g2s",
        "tags": []
      },
      "source": [
        "# Part 2: Multiple Regression Application and Diagnostics (Wine)\n",
        "We can switch gears a little here. We'll still use the same dataset for multiple regression, but let's review a little theory before putting it into practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "mecUCxBF2g2t",
        "tags": []
      },
      "source": [
        "## Notation and review\n",
        "\n",
        "Here is a quick summary of how we can formulate and approach the linear regression problem. For a more detailed derivation, see these [accompanying notes](./notes-linreg.ipynb).\n",
        "\n",
        "Your data consists of $m$ observations and $n+1$ variables. One of these variables is the _response_ variable, $y$, which you want to predict from the other $n$ variables, $\\{x_0, \\ldots, x_{n-1}\\}$. You wish to fit a _linear model_ of the following form to these data,\n",
        "\n",
        "$$y_i \\approx x_{i,0} \\theta_0 + x_{i,1} \\theta_1 + \\cdots + x_{i,n-1} \\theta_{n-1} + \\theta_n,$$\n",
        "\n",
        "where $\\{\\theta_j | 0 \\leq j \\leq n\\}$ is the set of unknown coefficients. Your modeling task is to choose values for these coefficients that \"best fit\" the data.\n",
        "\n",
        "If we further define a set of dummy variables, $x_{i, n} \\equiv 1.0$, associated with the $\\theta_n$ parameter, then the model can be written more compactly in matrix notation as\n",
        "\n",
        "$$\n",
        "  y \\approx X \\theta,\n",
        "$$\n",
        "\n",
        "where we will refer to $X$ as the (input) data matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "h2gmGzay2g2t",
        "tags": []
      },
      "source": [
        "Visually, you can also arrange the observations into a tibble like this one:\n",
        "\n",
        "|     y      | x<sub>0</sub> | x<sub>1</sub> | $\\cdots$ | x<sub>n-1</sub> | x<sub>n</sub> |\n",
        "|:----------:|:-------------:|:-------------:|:--------:|:---------------:|:-------------:|\n",
        "|   $y_0$    |   $x_{0,1}$   |   $x_{0,2}$   | $\\cdots$ |   $x_{0,n-1}$   |      1.0      |\n",
        "|   $y_1$    |   $x_{1,1}$   |   $x_{1,2}$   | $\\cdots$ |   $x_{1,n-1}$   |      1.0      |\n",
        "|   $y_2$    |   $x_{2,1}$   |   $x_{2,2}$   | $\\cdots$ |   $x_{2,n-1}$   |      1.0      |\n",
        "|  $\\vdots$  |   $\\vdots$    |   $\\vdots$    | $\\vdots$ |    $\\vdots$     |      1.0      |\n",
        "|  $y_{m-1}$ |  $x_{m-1,1}$  |  $x_{m-1,2}$  | $\\cdots$ |  $x_{m-1,n-1}$  |      1.0      |\n",
        "\n",
        "This tibble includes an extra column (variable), $x_n$, whose entries are all equal to 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "aK-k7qUH2g2t",
        "tags": []
      },
      "source": [
        "**Disclaimer** For the exercises in this part of the notebook, we will start over with the Wine Quality dataset, focusing on `quality` as the main target `y`. The other variables will represent `x` variables. We will go through a few different ways in which to perform the regression. The next cell should display that again. This time, as we are getting more advanced, let's go ahead and create a train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "editable": false,
        "id": "b1UQEzaz2g2u",
        "outputId": "c52d967d-8ff9-443d-9b06-9ae6b481895e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "hDyU6gJp2g2u",
        "tags": []
      },
      "source": [
        "**Exercise 4** (20 points). Import metrics from [Sklearn.model_selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Create dataframe `X`, having dropped the `quality` and `color` variables. Create dataframe `y` with just the `quality` column. Split the data into a train and test sets, with an 70% and 30% split.  Please also set the `random_state` part of the function to 42 for reproducability and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMr3JltZ2g2u",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "##YOUR ANSWER HERE\n",
        "#BEGIN SOLUTION\n",
        "X = df.copy()\n",
        "X.drop(columns=['quality', 'color'], axis=1, inplace=True)\n",
        "y = df['quality']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "BV-Z3GPN2g2v",
        "outputId": "cd00ca90-88dd-4386-e0dc-50d5c4f31767",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Test cell: `shape_data`\n",
        "assert x_train.shape == (4547, 11), \"Shape of x_train is {} instead of {}.  Be sure to use random_state = 42.\".format(x_train.shape, '(4547, 11)')\n",
        "assert y_train.shape == (4547,), \"Alpha is {} instead of {}. Be sure to use random_state = 42.\".format(y_train.shape, '(4547,)')\n",
        "assert x_test.shape == (1950, 11), \"Alpha is {} instead of {}. Be sure to use random_state = 42.\".format(x_test.shape, '(1950, 11)')\n",
        "assert y_test.shape == (1950,), \"Alpha is {} instead of {}.Be sure to use random_state = 42.\".format(y_test.shape, '(1950,)')\n",
        "print(\"\\n(Passed! You got 20 points!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "3SbKTmwk2g2v",
        "tags": []
      },
      "source": [
        "## Another Way\n",
        "\n",
        "Now that we have our splits, let's acknowledge something. While scikit is a great tool for many different Machine Learning models, it is important not to forget about other packages, as they occasionally have better ways to perform the same same functions. Take a little time to see how [Statsmodel](https://www.datarobot.com/blog/multiple-regression-using-statsmodels/#:~:text=You%20can%20also%20use%20the%20formulaic%20interface%20of,smf%20%23%20formula%3A%20response%20~%20predictor%20%2B%20predictor) works. The main reason to use it is that the summary information from our model can be much easier to extract and interpret. Let's try it out using what we know."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "4aU9FX4y2g2v",
        "tags": []
      },
      "source": [
        "**Exercise 5** (50 points). Create a function that will train an OLS model using statsmodel functions and the training sets so we can better see the summary and coefficients of each variable. Be sure to [add a constant](https://www.statsmodels.org/stable/generated/statsmodels.tools.tools.add_constant.html) to our X matrix as shown above in the \"Notation and Review\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "FU8-ox572g2v",
        "outputId": "fd431218-c91a-458f-835f-25b0ff2726dd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mlr(X, y):\n",
        "    import statsmodels.api as sm\n",
        "    ##YOUR CODE HERE##\n",
        "    #BEGIN SOLUTION#\n",
        "    X = sm.add_constant(X)\n",
        "    est = sm.OLS(y, X).fit()\n",
        "    return est\n",
        "    #END SOLUTION#\n",
        "mlin_reg = mlr(x_train, y_train)\n",
        "mlin_reg.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "zTlZI3MQ2g2x",
        "outputId": "bf33ba27-a798-445d-fee8-aea0c4cacfeb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Test cell: `statsmodels`\n",
        "assert str(type(mlin_reg)) == \"<class 'statsmodels.regression.linear_model.RegressionResultsWrapper'>\", \"Not a regression model\"\n",
        "test = (mlin_reg.summary().tables[1])\n",
        "test = pd.DataFrame(test)\n",
        "ts1 = pd.Series(test.iloc[:,1].values, index=test.iloc[:,0])\n",
        "ts1 = ts1.astype(np.float16, errors='ignore')\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/1_2_5_sol.txt'\n",
        "sol = pd.read_fwf(url, header= None)\n",
        "sol = sol.set_index(sol.columns[0], inplace=False)\n",
        "sol = sol.rename_axis(index=None, columns=['Index'])\n",
        "columns = ['coef']\n",
        "sol.columns= columns\n",
        "ts2 = pd.Series(sol.iloc[:,0].values, index=sol.index)\n",
        "\n",
        "try:\n",
        "    ts1.equals(ts2) #, check_exact=False, atol=0.001\n",
        "    print(\"\\n(Passed! You got 50 points!)\")\n",
        "except AssertionError as e:\n",
        "    print(\"Models are not equal:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "hkvs6CW22g2x",
        "tags": []
      },
      "source": [
        "# Readjust Model\n",
        "**Exercise 6** (60 points) It seems from the above example that there are a few variables that have no statistical significance being in the model. As such let's take out `chlorides` and `citric acid` to ensure that we are keeping everything statistically significant. Then we'll re-run everything and check the summary output again.\n",
        "\n",
        "Checklist:\n",
        "* Create `X_adj` from the original df, dropping `quality`, `color`, `chlorides`, and `citric acid`\n",
        "* Create `x_train_adj`, `x_test_adj`, `y_train_adj`, `y_test_adj` using a 70/30 split and random state 42\n",
        "* Create `mlin_reg_adj` by running our mlr function on your adjusted training sets\n",
        "* Show a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "E0AaDmo-2g2y",
        "outputId": "12e075ab-88bf-4627-b5ea-268f5fbc5655",
        "tags": []
      },
      "outputs": [],
      "source": [
        "##YOUR CODE HERE##\n",
        "#BEGIN SOLUTION#\n",
        "X_adj = df.copy()\n",
        "X_adj.drop(columns=['quality', 'color', 'chlorides', 'citric acid'], axis=1, inplace=True)\n",
        "x_train_adj, x_test_adj, y_train_adj, y_test_adj = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "mlin_reg_adj = mlr(x_train_adj, y_train_adj)\n",
        "mlin_reg_adj.summary()\n",
        "#END SOLUTION#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "vQqoOynV2g2y",
        "outputId": "675a1539-0706-45e3-e80d-97481324ea98",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Test cell: `statsmodels adj`\n",
        "assert str(type(mlin_reg_adj)) == \"<class 'statsmodels.regression.linear_model.RegressionResultsWrapper'>\", \"Not a regression model\"\n",
        "test = (mlin_reg_adj.summary().tables[1])\n",
        "test = pd.DataFrame(test)\n",
        "ts1 = pd.Series(test.iloc[:,1].values, index=test.iloc[:,0])\n",
        "ts1 = ts1.astype(np.float16, errors='ignore')\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/1_2_5_sol.txt'\n",
        "sol = pd.read_fwf(url, header= None)\n",
        "sol = sol.set_index(sol.columns[0], inplace=False)\n",
        "sol = sol.rename_axis(index=None, columns=['Index'])\n",
        "columns = ['coef']\n",
        "sol.columns= columns\n",
        "ts2 = pd.Series(sol.iloc[:,0].values, index=sol.index)\n",
        "\n",
        "try:\n",
        "    ts1.equals(ts2) #, check_exact=False, atol=0.001\n",
        "    print(\"\\n(Passed! You got 60 points!)\")\n",
        "except AssertionError as e:\n",
        "    print(\"Models are not equal:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1-pEh0E2g2z",
        "tags": []
      },
      "source": [
        "**Exercise 7** (10 points) Now let's see what it looks like to run a stepwise regression. The underlying goal of stepwise regression is, through a series of tests (e.g. F-tests, t-tests) to find a set of independent variables that significantly influence the dependent variable. This is done through iteration. Stepwise regression can be achieved either by trying out one independent variable at a time and including it in the regression model if it is statistically significant or by including all potential independent variables in the model and eliminating those that are not statistically significant.\n",
        "\n",
        "* **Forward selection** begins with no variables in the model, tests each variable as it is added to the model, then keeps those that are deemed most statistically significant—repeating the process until the results are optimal.\n",
        "* **Backward elimination** starts with a set of independent variables, deleting one at a time, then testing to see if the removed variable is statistically significant.\n",
        "* **Bidirectional elimination** is a combination of the first two methods that test which variables should be included or excluded.\n",
        "\n",
        "The below runs a stepwise using the [Sequential Feature Selector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) as a part of Scikit, which can come in handy for this kind of analysis. Let's see how a forward selection compares to the models we've already created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "editable": false,
        "id": "rL6h5dIi2g2z",
        "outputId": "816e85ae-d52b-4b78-c09f-95da5371bb4b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "# Perform stepwise regression\n",
        "sfs = SequentialFeatureSelector(LinearRegression(),\n",
        "\t\t\t\t\t\t\t\tn_features_to_select=8,\n",
        "\t\t\t\t\t\t\t\tdirection='forward',\n",
        "                                scoring='r2',\n",
        "\t\t\t\t\t\t\t\tcv=None)\n",
        "selected_features = sfs.fit(X, y)\n",
        "\n",
        "df_selected = df[selected_features.get_feature_names_out()]\n",
        "display(df_selected)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "        df_selected, y,\n",
        "        test_size=0.3,\n",
        "        random_state=42)\n",
        "\n",
        "# Fit a linear regression model using the selected features\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred_step = reg.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "meanAbErr = mean_absolute_error(y_test, y_pred_step)\n",
        "meanSqErr = mean_squared_error(y_test, y_pred_step)\n",
        "rootMeanSqErr = mean_squared_error(y_test, y_pred_step, squared = False)\n",
        "rsquared = r2_score(y_test, y_pred_step)\n",
        "print('R squared:', rsquared)\n",
        "print('Mean Absolute Error:', meanAbErr)\n",
        "print('Mean Square Error:', meanSqErr)\n",
        "print('Root Mean Square Error:', rootMeanSqErr)\n",
        "print('(Passed! You got 10 points!)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "06mtPXVw2g20",
        "tags": []
      },
      "source": [
        "Not much better... I wonder if there is something we already know from Applied ML1 that we can use..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "4VM6rBz62g20",
        "tags": []
      },
      "source": [
        "# Power Boost\n",
        "**Exercise 7** (70 points) In ML1 we learned about boosted models, and specifically XGBoost. Interestingly enough, this kind of model can also be used in a regression problem. Here we will attempt to do so in a similar way as we did in previous problems.\n",
        "\n",
        "Checklist:\n",
        "* Import our GradientBoostingRegressor from sklearn.ensemble, cross_val_score from sklearn.model_selection, and RepeatedKFold from sklearn.model_selection\n",
        "* Create `xg` by using the imported GradientBoostedRegressor\n",
        "* Fit model `xg` with `x_train_adj` and `y_train_adj` that we made earlier\n",
        "* Create `y_pred_xg` by predicting on `x_test_adj`\n",
        "* Create `meanAbErr_xg` using mean_absolute_error function (Refer to Exercise 3, can copy/paste then change to refer to `y_test_adj` and `y_pred_xg`)\n",
        "* Create `meanSqErr_xg` using mean_squared_error function (Refer to Exercise 3, can copy/paste then change to refer to `y_test_adj` and `y_pred_xg`)\n",
        "* Create `r2_xg` using mean_squared_error function (Refer to Exercise 3, can copy/paste then change to refer to `y_test_adj` and `y_pred_xg`)\n",
        "* Create `meanSqErr_xg` using r2_score function (Refer to Exercise 3, can copy/paste then change to refer to `y_test_adj` and `y_pred_xg`)\n",
        "\n",
        "* Print `r2_xg`, `meanAbErr_xg`, `meanSqErr_xg`, `rootMeanSqErr_xg` with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9DcTQ4L2g20",
        "outputId": "7980e86b-700b-483c-b9b6-4031e66c7a0c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "##YOUR ANSWER HERE\n",
        "#BEGIN SOLUTION\n",
        "##Fit Regression Model\n",
        "xg = GradientBoostingRegressor().fit(x_train_adj, y_train_adj)\n",
        "\n",
        "# Predict\n",
        "y_pred_xg = xg.predict(x_test_adj)\n",
        "\n",
        "# Evaluate the model performance\n",
        "meanAbErr_xg = mean_absolute_error(y_test_adj, y_pred_xg)\n",
        "meanSqErr_xg = mean_squared_error(y_test_adj, y_pred_xg)\n",
        "rootMeanSqErr_xg = mean_squared_error(y_test_adj, y_pred_xg, squared = False)\n",
        "r2_xg = r2_score(y_test_adj, y_pred_xg)\n",
        "print('R squared:', r2_xg)\n",
        "print('Mean Absolute Error:', meanAbErr_xg)\n",
        "print('Mean Square Error:', meanSqErr_xg)\n",
        "print('Root Mean Square Error:', rootMeanSqErr_xg)\n",
        "print('(Passed! You got 10 points!)')\n",
        "#END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "id": "G64Ar8Sd2g21",
        "outputId": "00c9c7f1-ec25-4cca-cbc8-e323bb5c166e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Test cell: `xgboost`\n",
        "url = 'https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/1_2_7_sol.txt'\n",
        "sol = pd.read_csv(url, sep=\":\", header=None)\n",
        "sol = sol.transpose()\n",
        "headers = sol.iloc[0].values\n",
        "sol.columns = headers\n",
        "sol.drop(index=0, axis=0, inplace=True)\n",
        "maesol = sol['Mean Absolute Error'].sum()\n",
        "msesol = sol['Mean Square Error'].sum()\n",
        "rmsesol = sol['Root Mean Square Error'].sum()\n",
        "r2sol = sol['R squared'].sum()\n",
        "# assert isclose(meanAbErr_xg, maesol, rtol=.01), \"Alpha is {} instead of {}.\".format(meanAbErr_xg, maesol)\n",
        "# assert isclose(meanSqErr_xg, msesol, rtol=.01), \"Alpha is {} instead of {}.\".format(meanSqErr_xg, msesol)\n",
        "# assert isclose(rootMeanSqErr_xg, rmsesol, rtol=.01), \"Alpha is {} instead of {}.\".format(rootMeanSqErr_xg, rmsesol)\n",
        "# assert isclose(r2_xg, r2sol, rtol=.01), \"Alpha is {} instead of {}.\".format(r2_xg, r2sol)\n",
        "np.testing.assert_almost_equal(meanAbErr_xg, maesol, decimal=2, err_msg=\"Alpha is {} instead of {}.\".format(meanAbErr_xg, maesol), verbose=True)\n",
        "np.testing.assert_almost_equal(meanSqErr_xg, msesol, decimal=2, err_msg=\"Alpha is {} instead of {}.\".format(meanSqErr_xg, msesol), verbose=True)\n",
        "np.testing.assert_almost_equal(rootMeanSqErr_xg, rmsesol, decimal=2, err_msg=\"Alpha is {} instead of {}.\".format(rootMeanSqErr_xg, rmsesol), verbose=True)\n",
        "np.testing.assert_almost_equal(r2_xg, r2sol, decimal=2, err_msg=\"Alpha is {} instead of {}.\".format(r2_xg, r2sol), verbose=True)\n",
        "print(\"\\n(Passed! You got 70 points!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "editable": false,
        "id": "TMAkIO_v2g21",
        "outputId": "1c9909fb-3635-47e6-fbc5-d918a70a4119",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "params = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": 4,\n",
        "    \"min_samples_split\": 5,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"loss\": \"squared_error\",\n",
        "}\n",
        "feature_importance = xg.feature_importances_\n",
        "test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
        "for i, y_pred_xg in enumerate(xg.staged_predict(x_test_adj)):\n",
        "    test_score[i] = mean_squared_error(y_test_adj, y_pred_xg)\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(\"Deviance\")\n",
        "plt.plot(\n",
        "    np.arange(params[\"n_estimators\"]) + 1,\n",
        "    xg.train_score_,\n",
        "    \"b-\",\n",
        "    label=\"Training Set Deviance\",\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
        ")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(\"Boosting Iterations\")\n",
        "plt.ylabel(\"Deviance\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "feature_importance = xg.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
        "plt.yticks(pos, np.array(x_test_adj.columns)[sorted_idx])\n",
        "plt.title(\"Feature Importance (MDI)\")\n",
        "\n",
        "result = permutation_importance(xg, x_test_adj, y_test_adj, n_repeats=10, random_state=42, n_jobs=2)\n",
        "sorted_idx = result.importances_mean.argsort()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(\n",
        "    result.importances[sorted_idx].T,\n",
        "    vert=False,\n",
        "    labels=np.array(x_test_adj.columns)[sorted_idx],\n",
        ")\n",
        "plt.title(\"Permutation Importance (test set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "P3UghccZ2g21",
        "tags": []
      },
      "source": [
        "While this does not seem like a tremendously good model, having only a modest R Squared of 35%, what is important to realize is that some real world problems do not always have wildly impressive stats to go with them. That does not mean that they are not valuable and that information cannot be gleaned from them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "37MHRlAi2g21",
        "tags": []
      },
      "source": [
        "**Congrats!** If you've gotten this far without errors, you're ready to submit your notebook! Be sure to Restart the Kernel and Run All Cells from the beginning."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
