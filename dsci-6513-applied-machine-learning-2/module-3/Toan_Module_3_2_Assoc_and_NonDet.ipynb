{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27173eec",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Association and Non-Deterministic Models â€” STARTER NOTEBOOK\n",
    "\n",
    "**Applied Machine Learning 2 @ Newman University**\n",
    "\n",
    "*Prof. Ricky Boyer*\n",
    "\n",
    "**Linh Toan**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e3dc3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Important note!** Before you turn in this lab notebook, make sure everything runs as expected:\n",
    "\n",
    "- First, **restart the kernel** -- in the menubar, select Kernel$\\rightarrow$Restart.\n",
    "- Then **run all cells** -- in the menubar, select Cell$\\rightarrow$Run All.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01412fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Part 1: Association rule mining\n",
    "\n",
    "In this notebook, you'll implement the basic pairwise association rule mining algorithm. This is a business problem that comes up frequently. The ability to determine how purchases of certain products affect the likelihood of purchasing other products has immediately actionable implications.\n",
    "\n",
    "To keep the implementation simple, you will apply your implementation to a simplified dataset, namely, letters (\"items\") in words (\"receipts\" or \"baskets\"). We'll be using this as a proxy for educational purposes, then move on to a typical association problem with grocery carts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d534d30",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Let's say you have a fragment of text in some language. You wish to know whether there are association rules among the letters that appear in a word. In this problem:\n",
    "\n",
    "- Words are \"receipts\"\n",
    "- Letters within a word are \"items\"\n",
    "\n",
    "You want to know whether there are _association rules_ of the form, $a \\implies b$, where $a$ and $b$ are letters. You will write code to do that by calculating for each rule its _confidence_, $\\mathrm{conf}(a \\implies b)$, which is an estimate of the conditional probability of $b$ given $a$, or $\\mathrm{Pr}[b \\,|\\, a]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e42bdc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Sample text input\n",
    "\n",
    "Let's carry out this analysis on a \"dummy\" text fragment, which graphic designers refer to as the [_lorem ipsum_](https://en.wikipedia.org/wiki/Lorem_ipsum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e23665f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters:\n",
      "  \n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium,  ...\n"
     ]
    }
   ],
   "source": [
    "latin_text = \"\"\"\n",
    "Sed ut perspiciatis, unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium, totam\n",
    "rem aperiam eaque ipsa, quae ab illo inventore\n",
    "veritatis et quasi architecto beatae vitae dicta\n",
    "sunt, explicabo. Nemo enim ipsam voluptatem, quia\n",
    "voluptas sit, aspernatur aut odit aut fugit, sed\n",
    "quia consequuntur magni dolores eos, qui ratione\n",
    "voluptatem sequi nesciunt, neque porro quisquam est,\n",
    "qui dolorem ipsum, quia dolor sit amet consectetur\n",
    "adipisci[ng] velit, sed quia non numquam [do] eius\n",
    "modi tempora inci[di]dunt, ut labore et dolore\n",
    "magnam aliquam quaerat voluptatem. Ut enim ad minima\n",
    "veniam, quis nostrum exercitationem ullam corporis\n",
    "suscipit laboriosam, nisi ut aliquid ex ea commodi\n",
    "consequatur? Quis autem vel eum iure reprehenderit,\n",
    "qui in ea voluptate velit esse, quam nihil molestiae\n",
    "consequatur, vel illum, qui dolorem eum fugiat, quo\n",
    "voluptas nulla pariatur?\n",
    "\n",
    "At vero eos et accusamus et iusto odio dignissimos\n",
    "ducimus, qui blanditiis praesentium voluptatum\n",
    "deleniti atque corrupti, quos dolores et quas\n",
    "molestias excepturi sint, obcaecati cupiditate non\n",
    "provident, similique sunt in culpa, qui officia\n",
    "deserunt mollitia animi, id est laborum et dolorum\n",
    "fuga. Et harum quidem rerum facilis est et expedita\n",
    "distinctio. Nam libero tempore, cum soluta nobis est\n",
    "eligendi optio, cumque nihil impedit, quo minus id,\n",
    "quod maxime placeat, facere possimus, omnis voluptas\n",
    "assumenda est, omnis dolor repellendus. Temporibus\n",
    "autem quibusdam et aut officiis debitis aut rerum\n",
    "necessitatibus saepe eveniet, ut et voluptates\n",
    "repudiandae sint et molestiae non recusandae. Itaque\n",
    "earum rerum hic tenetur a sapiente delectus, ut aut\n",
    "reiciendis voluptatibus maiores alias consequatur\n",
    "aut perferendis doloribus asperiores repellat.\n",
    "\"\"\"\n",
    "\n",
    "print(\"First 100 characters:\\n  {} ...\".format(latin_text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5864a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Data cleaning.** Like most data in the real world, this dataset is noisy. It has both uppercase and lowercase letters, words have repeated letters, and there are all sorts of non-alphabetic characters. For our analysis, we should keep all the letters and spaces (so we can identify distinct words), but we should ignore case and ignore repetition within a word.\n",
    "\n",
    "For example, the eighth word of this text is \"error.\" As an _itemset_, it consists of the three unique letters, $\\{e, o, r\\}$. That is, treat the word as a set, meaning you only keep the unique letters.\n",
    "\n",
    "This itemset has three possible _itempairs_: $\\{e, o\\}$, $\\{e, r\\}$, and $\\{o, r\\}$.\n",
    "\n",
    "Start by writing some code to help \"clean up\" the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422c578",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 0** (`normalize_string_test`: 15 points). Complete the following function, `normalize_string(s)`, which should take a string (`str` object) `s` as input and returns a new string with all characters converted to lowercase and all non-alphabetic, non-space characters removed.\n",
    "\n",
    "> Scanning the sample text, `latin_text`, you may see things that look like special cases. For instance, `inci[di]dunt` and `[do]`. For these, simply remove the non-alphabetic characters and only separate the words if there is explicit whitespace.\n",
    ">\n",
    "> For instance, `inci[di]dunt` would become `incididunt` (as a single word) and `[do]` would become `do` as a standalone word because the original string has whitespace on either side. A period or comma without whitespace would, similarly, just be treated as a non-alphabetic character inside a word _unless_ there is explicit whitespace. So `e pluribus.unum basium` would become `e pluribusunum basium` even though your common-sense understanding might separate `pluribus` and `unum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abb50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium,  ...\n",
      "=> \n",
      "sed ut perspiciatis unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium  ...\n"
     ]
    }
   ],
   "source": [
    "def normalize_string(s):\n",
    "    assert type (s) is str\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    essential_chars = [c for c in s.lower() if c.isalpha() or c.isspace()]\n",
    "    return ''.join(essential_chars)\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "# Demo:\n",
    "print(latin_text[:100], \"...\\n=>\", normalize_string(latin_text[:100]), \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721d7148",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed! You got 15 points!)\n"
     ]
    }
   ],
   "source": [
    "# `normalize_string_test`: Test cell\n",
    "norm_latin_text = normalize_string(latin_text)\n",
    "\n",
    "assert type(norm_latin_text) is str\n",
    "assert len(norm_latin_text) == 1694\n",
    "assert all([c.isalpha() or c.isspace() for c in norm_latin_text])\n",
    "assert norm_latin_text == norm_latin_text.lower()\n",
    "\n",
    "print(\"\\n(Passed! You got 15 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c15cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 1** (`get_normalized_words_test`: 15 points). Implement the following function, `get_normalized_words(s)`: given a string (`str` object) `s`, returns a list of its words, normalized per the definition of `normalize_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc67254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five words:\n",
      "['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n"
     ]
    }
   ],
   "source": [
    "def get_normalized_words(s):\n",
    "    assert type (s) is str\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    words = normalize_string(s)\n",
    "    return words.split()\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Demo:\n",
    "print (\"First five words:\\n{}\".format (get_normalized_words (latin_text)[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2112cbb8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed! Congrats you got 15 points!)\n"
     ]
    }
   ],
   "source": [
    "# `get_normalized_words_test`: Test cell\n",
    "norm_latin_words = get_normalized_words(norm_latin_text)\n",
    "\n",
    "assert len(norm_latin_words) == 250\n",
    "for i, w in [(20, 'illo'), (73, 'eius'), (144, 'deleniti'), (248, 'asperiores')]:\n",
    "    assert norm_latin_words[i] == w\n",
    "\n",
    "print (\"\\n(Passed! Congrats you got 15 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5fe5c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 2** (`make_itemsets_test`: 10 points). Implement a function, `make_itemsets`, that given a list of strings converts the characters of each string into an itemset, returning the list of itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ce8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_itemsets(words):\n",
    "    # YOUR CODE HERE\n",
    "    itemset = []\n",
    "    for el in words:\n",
    "        itemset.append(set(el))\n",
    "    return itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "811c9461",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148] dolores --> {'o', 's', 'd', 'r', 'l', 'e'}\n",
      "[80] dolore --> {'o', 'd', 'r', 'l', 'e'}\n",
      "[172] fuga --> {'f', 'g', 'a', 'u'}\n",
      "[156] non --> {'n', 'o'}\n",
      "[234] tenetur --> {'u', 'n', 'r', 't', 'e'}\n",
      "\n",
      "(Passed! You got 10 points!)\n"
     ]
    }
   ],
   "source": [
    "# `make_itemsets_test`: Test cell\n",
    "norm_latin_itemsets = make_itemsets(norm_latin_words)\n",
    "\n",
    "# Lists should have the same size\n",
    "assert len(norm_latin_itemsets) == len(norm_latin_words)\n",
    "\n",
    "# Test a random sample\n",
    "from random import sample\n",
    "for i in sample(range(len(norm_latin_words)), 5):\n",
    "    print('[{}]'.format(i), norm_latin_words[i], \"-->\", norm_latin_itemsets[i])\n",
    "    assert set(norm_latin_words[i]) == norm_latin_itemsets[i]\n",
    "print(\"\\n(Passed! You got 10 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54a1fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Aside: Default dictionaries\n",
    "\n",
    "The overall algorithm requires maintaining a table of item-pair (tuples) counts. It would be convenient to use a dictionary to store this table, where keys refer to item-pairs and the values are the counts.\n",
    "\n",
    "However, with Python's built-in dictionaries, you always to have to check whether a key exists before updating it. For example, consider this code fragment:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "D['existing-key'] += 1 # == 6\n",
    "D['new-key'] += 1  # Error: 'new-key' does not exist!\n",
    "```\n",
    "\n",
    "The second attempt causes an error because `'new-key'` is not yet a member of the dictionary. So, a more correct approach would be to do the following:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "if 'existing-key' not in D:\n",
    "    D['existing-key'] = 0\n",
    "D['existing-key'] += 1\n",
    "   \n",
    "if 'new-key' not in D:\n",
    "    D['new-key'] = 0\n",
    "D['new-key'] += 1\n",
    "```\n",
    "\n",
    "This pattern is so common that there is a special form of dictionary, called a _default dictionary_, which is available from the `collections` module: [`collections.defaultdict`](https://docs.python.org/3/library/collections.html?highlight=defaultdict#collections.defaultdict).\n",
    "\n",
    "When you create a default dictionary, you need to provide a \"factory\" function that the dictionary can use to create an initial value when the key does *not* exist. For instance, in the preceding example, when the key was not present the code creates a new key with the initial value of an integer zero (0). Indeed, this default value is the one you get when you call `int()` with no arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d194605e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print (int ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b8aca5c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'existing-key': 6, 'new-key': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "D2 = defaultdict (int) # Empty dictionary\n",
    "\n",
    "D2['existing-key'] = 5 # Create one key-value pair\n",
    "\n",
    "D2['existing-key'] += 1 # Update\n",
    "D2['new-key'] += 1\n",
    "\n",
    "print (D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a839aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 3** (`update_pair_counts_test`: 15 points). Start by implementing a function that enumerates all item-pairs within an itemset and updates a table in-place that tracks the counts of those item-pairs.\n",
    "\n",
    "You may assume all items in the given itemset (`itemset` argument) are distinct, i.e., that you may treat it as you would any set-like collection. You may also assume `pair_counts` is a default dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dec4c741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combo =  <itertools.combinations object at 0x10dfdcbd0>\n",
      "c =  ('r', 'o')\n",
      "c =  ('r', 'e')\n",
      "c =  ('o', 'e')\n",
      "3\n",
      "combo =  <itertools.combinations object at 0x10de69a30>\n",
      "c =  ('d', 'l')\n",
      "c =  ('d', 'r')\n",
      "c =  ('d', 'o')\n",
      "c =  ('l', 'r')\n",
      "c =  ('l', 'o')\n",
      "c =  ('r', 'o')\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "itemset_1 = set(\"error\")\n",
    "itemset_2 = set(\"dolor\")\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "x = update_pair_counts(pair_counts, itemset_1)\n",
    "print(len(x))\n",
    "x2 = update_pair_counts(pair_counts, itemset_2)\n",
    "print(len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2feeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations # Hint!\n",
    "\n",
    "def update_pair_counts (pair_counts, itemset):\n",
    "    \"\"\"\n",
    "    Updates a dictionary of pair counts for\n",
    "    all pairs of items in a given itemset.\n",
    "    \"\"\"\n",
    "    assert type (pair_counts) is defaultdict\n",
    "    # YOUR CODE HERE\n",
    "    combo = combinations(itemset, 2)\n",
    "    for c in combo:\n",
    "        print(\"c = \", c)\n",
    "        pair_counts[c] += 1\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "34222916",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pair_counts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      6\u001b[0m update_pair_counts(pair_counts, itemset_1)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair_counts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      8\u001b[0m update_pair_counts(pair_counts, itemset_2)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair_counts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# `update_pair_counts_test`: Test cell\n",
    "itemset_1 = set(\"error\")\n",
    "itemset_2 = set(\"dolor\")\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "update_pair_counts(pair_counts, itemset_1)\n",
    "assert len(pair_counts) == 6\n",
    "update_pair_counts(pair_counts, itemset_2)\n",
    "assert len(pair_counts) == 16\n",
    "\n",
    "print('\"{}\" + \"{}\"\\n==> {}'.format (itemset_1, itemset_2, pair_counts))\n",
    "for a, b in pair_counts:\n",
    "    assert (b, a) in pair_counts\n",
    "    assert pair_counts[(a, b)] == pair_counts[(b, a)]\n",
    "    \n",
    "print (\"\\n(Passed! You got 15 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74207f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 4** (`update_item_counts_test`: 15 points). Implement a procedure that, given an itemset, updates a table to track counts of each item.\n",
    "\n",
    "As with the previous exercise, you may assume all items in the given itemset (`itemset`) are distinct, i.e., that you may treat it as you would any set-like collection. You may also assume the table (`item_counts`) is a default dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0270202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_item_counts(item_counts, itemset):\n",
    "    for item in itemset:\n",
    "        item_counts[item] += 1\n",
    "    return item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4012c731",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed! You got 15 points!)\n"
     ]
    }
   ],
   "source": [
    "# `update_item_counts_test`: Test cell\n",
    "itemset_1 = set(\"error\")\n",
    "itemset_2 = set(\"dolor\")\n",
    "\n",
    "item_counts = defaultdict(int)\n",
    "update_item_counts(item_counts, itemset_1)\n",
    "assert len(item_counts) == 3\n",
    "update_item_counts(item_counts, itemset_2)\n",
    "assert len(item_counts) == 5\n",
    "\n",
    "assert item_counts['d'] == 1\n",
    "assert item_counts['e'] == 1\n",
    "assert item_counts['l'] == 1\n",
    "assert item_counts['o'] == 2\n",
    "assert item_counts['r'] == 2\n",
    "\n",
    "print(\"\\n(Passed! You got 15 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b7b84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 5** (`filter_rules_by_conf_test`: 20 points). Given tables of item-pair counts and individual item counts, as well as a confidence threshold, return the rules that meet the threshold. The returned rules should be in the form of a dictionary whose key is the tuple, $(a, b)$ corresponding to the rule $a \\Rightarrow b$, and whose value is the confidence of the rule, $\\mathrm{conf}(a \\Rightarrow b)$.\n",
    "\n",
    "You may assume that if $(a, b)$ is in the table of item-pair counts, then both $a$ and $b$ are in the table of individual item counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "453d034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rules_by_conf (pair_counts, item_counts, threshold):\n",
    "    rules = {} # (item_a, item_b) -> conf (item_a => item_b)\n",
    "    # YOUR CODE HERE\n",
    "    for (a, b) in pair_counts:\n",
    "        confidence = pair_counts[(a, b)] / item_counts[a]\n",
    "        if confidence >= threshold:\n",
    "            rules[(a, b)] = confidence\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5408606d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these rules: {('man', 'woman'): 0.7142857142857143, ('red fish', 'blue fish'): 0.6363636363636364}\n",
      "\n",
      "(Passed! You got 20 points!)\n"
     ]
    }
   ],
   "source": [
    "# `filter_rules_by_conf_test`: Test cell\n",
    "pair_counts = {('man', 'woman'): 5,\n",
    "               ('bird', 'bee'): 3,\n",
    "               ('red fish', 'blue fish'): 7}\n",
    "item_counts = {'man': 7,\n",
    "               'bird': 9,\n",
    "               'red fish': 11}\n",
    "rules = filter_rules_by_conf (pair_counts, item_counts, 0.5)\n",
    "print(\"Found these rules:\", rules)\n",
    "assert ('man', 'woman') in rules\n",
    "assert ('bird', 'bee') not in rules\n",
    "assert ('red fish', 'blue fish') in rules\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece3812",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Aside: pretty printing the rules.** The output of rules above is a little messy; here's a little helper function that structures that output a little, which will be useful for both debugging and reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "16247597",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(man => woman) = 0.714\n",
      "conf(red fish => blue fish) = 0.636\n"
     ]
    }
   ],
   "source": [
    "def gen_rule_str(a, b, val=None, val_fmt='{:.3f}', sep=\" = \"):\n",
    "    text = \"{} => {}\".format(a, b)\n",
    "    if val:\n",
    "        text = \"conf(\" + text + \")\"\n",
    "        text += sep + val_fmt.format(val)\n",
    "    return text\n",
    "\n",
    "def print_rules(rules):\n",
    "    if type(rules) is dict or type(rules) is defaultdict:\n",
    "        from operator import itemgetter\n",
    "        ordered_rules = sorted(rules.items(), key=itemgetter(1), reverse=True)\n",
    "    else: # Assume rules is iterable\n",
    "        ordered_rules = [((a, b), None) for a, b in rules]\n",
    "    for (a, b), conf_ab in ordered_rules:\n",
    "        print(gen_rule_str(a, b, conf_ab))\n",
    "\n",
    "# Demo:\n",
    "print_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6353f2f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 6** (`find_assoc_rules_test`: 20 points). Using the building blocks you implemented above, complete a function `find_assoc_rules` so that it implements the basic association rule mining algorithm and returns a dictionary of rules.\n",
    "\n",
    "In particular, your implementation may assume the following:\n",
    "\n",
    "1. As indicated in its signature, below, the function takes two inputs: `receipts` and `threshold`.\n",
    "1. The input, `receipts`, is a collection of itemsets: for every receipt `r` in `receipts`, `r` may be treated as a collection of unique items.\n",
    "2. The input `threshold` is the minimum desired confidence value. That is, the function should only return rules whose confidence is at least `threshold`.\n",
    "\n",
    "The returned dictionary, `rules`, should be keyed by tuples $(a, b)$ corresponding to the rule $a \\Rightarrow b$; each value should the the confidence $\\mathrm{conf}(a \\Rightarrow b)$ of the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def find_assoc_rules(receipts, threshold):\n",
    "    # YOUR CODE HERE\n",
    "    rules = defaultdict(int)\n",
    "    for r in receipts:\n",
    "        combo = combinations(r, 2)\n",
    "        conf_r1 = \n",
    "        confidence = \n",
    "        if confidence >= threshold:\n",
    "            rules[r] = confidence\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5765180d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original receipts as itemsets: [{'a', 'c', 'b'}, {'a', 'c'}, {'a'}]\n",
      "Resulting rules:\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m print_rules(rules)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rules\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m rules\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m rules\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m rules\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# `find_assoc_rules_test`: Test cell\n",
    "receipts = [set('abbc'), set('ac'), set('a')]\n",
    "rules = find_assoc_rules(receipts, 0.6)\n",
    "\n",
    "print(\"Original receipts as itemsets:\", receipts)\n",
    "print(\"Resulting rules:\")\n",
    "print_rules(rules)\n",
    "\n",
    "assert ('a', 'b') not in rules\n",
    "assert ('b', 'a') in rules\n",
    "assert ('a', 'c') in rules\n",
    "assert ('c', 'a') in rules\n",
    "assert ('b', 'c') in rules\n",
    "assert ('c', 'b') not in rules\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f2fc5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 7** (`latin_rules_test`: 10 points). For the Latin string, `latin_text`, use your `find_assoc_rules()` function to compute the rules whose confidence is at least 0.75. Store your result in a variable named `latin_rules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `latin_rules`:\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# Inspect your result:\n",
    "print_rules(latin_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134bca4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# `latin_rules_test`: Test cell\n",
    "assert len(latin_rules) == 10\n",
    "assert all([0.75 <= v <= 1.0 for v in latin_rules.values()])\n",
    "for ab in ['xe', 'qu', 'hi', 'xi', 'vt', 're', 've', 'fi', 'gi', 'bi']:\n",
    "    assert (ab[0], ab[1]) in latin_rules\n",
    "print(\"\\n(Passed! You got 10 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b075b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Putting it all together: Actual baskets!\n",
    "\n",
    "Let's take a look at some real data that [someone](http://www.salemmarafi.com/code/market-basket-analysis-with-r/) was kind enough to prepare for a similar exercise designed for the R programming environment.\n",
    "\n",
    "First, here's a code snippet to download the data, which is a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc7479",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get ('https://raw.githubusercontent.com/boyerr111/newmanu_AML2/master/datasets/groceries.csv')\n",
    "groceries_file = response.text  # or response.content for raw bytes\n",
    "\n",
    "print (groceries_file[0:250] + \"...\\n... (etc.) ...\") # Prints the first 250 characters only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79d841",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Each line of this file is some customer's shopping basket. The items that the customer bought are stored as a comma-separated list of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47862b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 8: Your task.** (`basket_rules_test`: 30 points). Your final task in this notebook is to mine this dataset for pairwise association rules. In particular, your code should produce (no pun intended!) a final dictionary, `basket_rules`, that meet these conditions (read carefully!):\n",
    "\n",
    "1. The keys are pairs $(a, b)$, where $a$ and $b$ are item names (as strings).\n",
    "2. The values are the corresponding confidence scores, $\\mathrm{conf}(a \\Rightarrow b)$.\n",
    "3. Only include rules $a \\Rightarrow b$ where item $a$ occurs at least `MIN_COUNT` times and $\\mathrm{conf}(a \\Rightarrow b)$ is at least `THRESHOLD`.\n",
    "\n",
    "Pay particular attention to Condition 3: not only do you have to filter by a confidence threshold, but you must exclude rules $a \\Rightarrow b$ where the item $a$ does not appear \"often enough.\" There is a code cell below that defines values of `MIN_COUNT` and `THRESHOLD`, but your code should work even if we decide to change those values later on.\n",
    "\n",
    "> _Aside_: Why would an analyst want to enforce Condition 3?\n",
    "\n",
    "Your solution can use the `groceries_file` string variable defined above as its starting point. And since it's in the same notebook, you may, of course, reuse any of the code you've written above as needed. Lastly, if you feel you need additional code cells, you can create them _after_ the code cell marked for your solution but _before_ the code marked, `### TEST CODE ###`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fe7a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Confidence threshold\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Only consider rules for items appearing at least `MIN_COUNT` times.\n",
    "MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d644e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "### BEGIN SOLUTION\n",
    "# Create itemsets and compute individual item counts\n",
    "\n",
    "# Search for an initial set of association rules\n",
    "\n",
    "# Filter those rules to exclude infrequent items\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7783c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### `basket_rules_test`: TEST CODE ###\n",
    "print(\"Found {} rules whose confidence exceeds {}.\".format(len(basket_rules), THRESHOLD))\n",
    "print(\"Here they are:\\n\")\n",
    "print_rules(basket_rules)\n",
    "\n",
    "assert len(basket_rules) == 19\n",
    "assert all([THRESHOLD <= v < 1.0 for v in basket_rules.values()])\n",
    "ans_keys = [(\"pudding powder\", \"whole milk\"), (\"tidbits\", \"rolls/buns\"), (\"cocoa drinks\", \"whole milk\"), (\"cream\", \"sausage\"), (\"rubbing alcohol\", \"whole milk\"), (\"honey\", \"whole milk\"), (\"frozen fruits\", \"other vegetables\"), (\"cream\", \"other vegetables\"), (\"ready soups\", \"rolls/buns\"), (\"cooking chocolate\", \"whole milk\"), (\"cereals\", \"whole milk\"), (\"rice\", \"whole milk\"), (\"specialty cheese\", \"other vegetables\"), (\"baking powder\", \"whole milk\"), (\"rubbing alcohol\", \"butter\"), (\"rubbing alcohol\", \"citrus fruit\"), (\"jam\", \"whole milk\"), (\"frozen fruits\", \"whipped/sour cream\"), (\"rice\", \"other vegetables\")]\n",
    "for k in ans_keys:\n",
    "    assert k in basket_rules\n",
    "\n",
    "print(\"\\n(Passed! You got 30 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cb7db",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Part 2: Non-deterministic - Markov Chains\n",
    "## Markov chain analysis of the US airport network\n",
    "\n",
    "One way to view the airline transportation infrastructure is in the form of a directed _network_ or _graph_, in which vertices are airports and edges are the direct-flight segments that connect them. For instance, if there is a direct flight from Atlanta's Hartsfield-Jackson International Airport (\"ATL\") to Los Angeles International Airport (\"LAX\"), then the airport network would have a directed edge from ATL to LAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267453ab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given the airport network, one question we might ask is, which airports are most critical to disruption of the overall network? That is, if an airport is shut down, thereby leading to all inbound and outbound flights being cancelled, will that catastrophic event have a big impact or a small impact on the overall network?\n",
    "\n",
    "You would expect \"importance\" to be related to whether an airport has lots of inbound or outgoing connections. In graph lingo, that's also called the _degree_ of a vertex or node. But if there are multiple routes that can work around a highly connected hub (i.e., a vertex with a high indegree or outdegree), that might not be the case. So let's try to use a PageRank-like scheme to see what we get and compare that to looking at degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291eca08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As it happens, the US Bureau of Transportation Statistics collects data on all flights originating or arriving in the United States. In this notebook, you'll use this data to build an airport network and then use Markov chain analysis to rank the networks by some measure of \"criticality.\"\n",
    "> Sources: This notebook is adapted from the following: https://www.mongodb.com/blog/post/pagerank-on-flights-dataset. The dataset is a small subset of what is available here: https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55018d1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## The formal analysis problem\n",
    "\n",
    "Let's model the analysis problem as follows.\n",
    "\n",
    "Consider a \"random flyer\" to be a person who arrives at an airport $i$, and then randomly selects any direct flight that departs from $i$ and arrives at $j$. We refer to the direct flight from $i$ to $j$ as the _flight segment_ $i \\rightarrow j$. Upon arriving at $j$, the flyer repeats the process of randomly selecting a new segment, $j \\rightarrow k$. He or she repeats this process forever.\n",
    "\n",
    "Let $x_i(t)$ be the probability that the flyer is at airport $i$ at time $t$. Take $t$ to be an integer count corresponding to the number of flight segments that the flyer has taken so far, starting at $t=0$. Let $p_{ij}$ be the probability of taking segment $i \\rightarrow j$, where $p_{ij} = 0$ means the segment $i \\rightarrow j$ is unavailable or does not exist. If there are $n$ airports in all, numbered from $0$ to $n-1$, then the probability that the flyer will be at airport $i$ at time $t+1$, given all the probabilities at time $t$, is\n",
    "\n",
    "$$\n",
    "  x_i(t+1) = \\sum_{j=0}^{n-1} p_{ji} \\cdot x_j(t).\n",
    "$$\n",
    "\n",
    "Let $P \\equiv [p_{ij}]$ be the matrix of transition probabilities and $x(t) = [x_i(t)]$ the column vector of prior probabilities. Then we can write the above more succinctly for all airports as the matrix-vector product,\n",
    "\n",
    "$$\n",
    "  x(t+1) = P^T x(t).\n",
    "$$\n",
    "\n",
    "Since $P$ is a probability transition matrix then there exists a steady-state distribution, $x^*$, which is the limit of $x(t)$ as $t$ goes to infinity:\n",
    "\n",
    "$$\\displaystyle \\lim_{t \\rightarrow \\infty} x(t) = x^* \\equiv [x_i^*].$$\n",
    "\n",
    "The larger $x_i^*$, the more likely it is that the random flyer is to be at airport $i$ in the steady state. Therefore, we can take the \"importance\" or \"criticality\" of airport $i$ in the flight network to be its steady-state probability, $x_i^*$.\n",
    "\n",
    "Thus, our data pre-processing goal is to construct $P$ and our analysis goal is to compute the steady-state probability distribution, $x^*$, for a first-order Markov chain system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821b9b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Modules\n",
    "\n",
    "For this part of the notebook, let's use Pandas for preprocessing the raw data and SciPy's sparse matrix libraries to implement the analysis.\n",
    "\n",
    "> One of the cells below defines a function, `spy()`, that can be used to visualize the non-zero structure of a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bfb07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def spy(A, figsize=(6, 6), markersize=0.5):\n",
    "    \"\"\"Visualizes a sparse matrix.\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.spy(A, markersize=markersize)\n",
    "    plt.show()\n",
    "    \n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def canonicalize_tibble(X):\n",
    "    var_names = sorted(X.columns)\n",
    "    Y = X[var_names].copy()\n",
    "    Y.sort_values(by=var_names, inplace=True)\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "    return Y\n",
    "\n",
    "def tibbles_are_equivalent (A, B):\n",
    "    A_canonical = canonicalize_tibble(A)\n",
    "    B_canonical = canonicalize_tibble(B)\n",
    "    cmp = A_canonical.eq(B_canonical)\n",
    "    return cmp.all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51010952",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "import io\n",
    "\n",
    "def download(file, local_dir=\"\", url_base=None, checksum=None):\n",
    "    local_file = \"{}{}\".format(local_dir, file)\n",
    "    if not os.path.exists(local_file):\n",
    "        url = \"{}{}\".format(url_base, file)\n",
    "        print(\"Downloading: {} ...\".format(url))\n",
    "        r = requests.get(url)\n",
    "        with open(local_file, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    if checksum is not None:\n",
    "        with io.open(local_file, 'rb') as f:\n",
    "            body = f.read()\n",
    "            body_checksum = hashlib.md5(body).hexdigest()\n",
    "            assert body_checksum == checksum, \\\n",
    "                \"Downloaded file '{}' has incorrect checksum: '{}' instead of '{}'\".format(local_file,\n",
    "                                                                                           body_checksum,\n",
    "                                                                                           checksum)\n",
    "    print(\"'{}' is ready!\".format(file))\n",
    "    \n",
    "URL_BASE = \"boyerr111/newmanu_AML2/raw/master/datasets/\"\n",
    "DATA_PATH = \"\"\n",
    "\n",
    "datasets = {'L_AIRPORT_ID.csv': 'e9f250e3c93d625cce92d08648c4bbf0',\n",
    "            'L_CITY_MARKET_ID.csv': 'f430a16a5fe4b9a849accb5d332b2bb8',\n",
    "            'L_UNIQUE_CARRIERS.csv': 'bebe919e85e2cf72e7041dbf1ae5794e',\n",
    "            'us-flights--2017-08.csv': 'eeb259c0cdd00ff1027261ca0a7c0332',\n",
    "            'flights_atl_to_lax_soln.csv': '4591f6501411de90af72693cdbcc08bb',\n",
    "            'origins_top10_soln.csv': 'de85c321c45c7bf65612754be4567086',\n",
    "            'dests_soln.csv': '370f4c632623616b3bf26b6f79993fe4',\n",
    "            'dests_top10_soln.csv': '4c7dd7edf48c4d62466964d6b8c14184',\n",
    "            'segments_soln.csv': '516a78d2d9d768d78bfb012b77671f38',\n",
    "            'segments_outdegree_soln.csv': 'b29d60151c617ebafd3a1c58541477c8'\n",
    "           }\n",
    "\n",
    "for filename, checksum in datasets.items():\n",
    "    download(filename, local_dir=DATA_PATH, url_base=URL_BASE, checksum=checksum)\n",
    "    \n",
    "print(\"\\n(All data appears to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c062a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Airport codes.** Let's begin by looking at airport codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e013064",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "airport_codes = pd.read_csv(\"{}{}\".format(DATA_PATH, 'L_AIRPORT_ID.csv'))\n",
    "airport_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ac9f9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Flight segments.** Next, let's load a file that contains all of US flights that were scheduled in August 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dea9b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "flights = pd.read_csv('{}{}'.format(DATA_PATH, 'us-flights--2017-08.csv'))\n",
    "print(\"Number of flight segments: {} [{:.1f} million]\".format (len(flights), len(flights)*1e-6))\n",
    "del flights['Unnamed: 7'] # Cleanup extraneous column\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13265e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Each row of this tibble is a _(direct) flight segment_, that is, a flight that left some origin and arrived at a destination on a certain date. As noted earlier, these segments cover a one-month period (August 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d45d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As a way of helping to familiarize you with this dataset, I've determined all direct flight segments that originated at Atlanta's Hartsfield-Jackson International Airport and traveled to Los Angeles International. The data is storein a dataframe named `flights_atl_to_lax`, which should be the corresponding subset of rows from `flights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b08eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def lookup_airport(index):\n",
    "    return airport_codes.loc[index, 'Code'], airport_codes.loc[index, 'Description']\n",
    "\n",
    "ATL_ID, ATL_DESC = lookup_airport(373)\n",
    "LAX_ID, LAX_DESC = lookup_airport(2765)\n",
    "\n",
    "print(\"{}: ATL -- {}\".format(ATL_ID, ATL_DESC))\n",
    "print(\"{}: LAX -- {}\".format(LAX_ID, LAX_DESC))\n",
    "print()\n",
    "\n",
    "is_atl_origin = (flights['ORIGIN_AIRPORT_ID'] == ATL_ID)\n",
    "is_lax_dest = (flights['DEST_AIRPORT_ID'] == LAX_ID)\n",
    "is_atl_to_lax = is_atl_origin & is_lax_dest\n",
    "flights_atl_to_lax = flights[is_atl_to_lax]\n",
    "\n",
    "# Displays a few of your results\n",
    "print(\"Your code found {} flight segments.\".format(len(flights_atl_to_lax)))\n",
    "display(flights_atl_to_lax.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9371d8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Aggregation.** Observe that an (origin, destination) pair may appear many times. That's because the dataset includes a row for _every_ direct flight that occurred historically and there may have been multiple such flights on a given day.\n",
    "\n",
    "However, for the purpose of this analysis, let's simplify the problem by collapsing _all_ historical segments $i \\rightarrow j$ into a single segment. Let's also do so in a way that preserves the number of times the segment occurred (i.e., the number of rows containing the segment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d87a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To accomplish this task, the following code cell uses the [`groupby()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) function available for Pandas tables and the [`count()`](http://pandas.pydata.org/pandas-docs/stable/groupby.html) aggregator in three steps:\n",
    "\n",
    "1. It considers just the flight date, origin, and destination columns.\n",
    "2. It _logically_ groups the rows having the same origin and destination, using `groupby()`.\n",
    "3. It then aggregates the rows, counting the number of rows in each (origin, destination) group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8aef6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "flights_cols_subset = flights[['FL_DATE', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']]\n",
    "segment_groups = flights_cols_subset.groupby(['ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID'], as_index=False)\n",
    "segments = segment_groups.count()\n",
    "segments.rename(columns={'FL_DATE': 'FL_COUNT'}, inplace=True)\n",
    "segments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f38b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As a last sanity check, let's verify that the counts are all at least 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da80a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert (segments['FL_COUNT'] > 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086111c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Actual (as opposed to \"all possible\") origins and destinations.** Although there are many possible airport codes stored in the `airport_codes` dataframe (over six thousand), only a subset appear as actual origins in the data. The following code cell determines the actual origins and prints their number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002919c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "origins = segments[['ORIGIN_AIRPORT_ID', 'FL_COUNT']].groupby('ORIGIN_AIRPORT_ID', as_index=False).sum()\n",
    "origins.rename(columns={'FL_COUNT': 'ORIGIN_COUNT'}, inplace=True)\n",
    "print(\"Number of actual origins:\", len(origins))\n",
    "origins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e685b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To get an idea of what airports are likely to be the most important in our Markov chain analysis, let's rank airports by the total number of _outgoing_ segments, i.e., flight segments that originate at the airport."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aec155",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 9** (`origin_ranks_test`: 30 points). Construct a dataframe, `origins_top10`, containing the top 10 airports in descending order of outgoing segments. This dataframe should have three columns:\n",
    "\n",
    "* `ID`: The ID of the airport\n",
    "* `Count`: Number of outgoing segments.\n",
    "* `Description`: The plaintext descriptor for the airport that comes from the `airport_codes` dataframe.\n",
    "\n",
    "> _Hint_: Look up and read about `numpy.argsort()`, which you can also apply to any Pandas Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank each origin\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# Prints the top 10, according to your calculation:\n",
    "origins_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b715db3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `origin_ranks_test`\n",
    "\n",
    "if False:\n",
    "    origins_top10.to_csv('origins_top10_soln.csv', index=False)\n",
    "origins_top10_soln = pd.read_csv('{}{}'.format(DATA_PATH, 'origins_top10_soln.csv'))\n",
    "\n",
    "print(\"=== Professor's solution ===\")\n",
    "display(origins_top10_soln)\n",
    "    \n",
    "assert tibbles_are_equivalent(origins_top10, origins_top10_soln), \\\n",
    "       \"Your table does not have the same entries as the solution.\"\n",
    "\n",
    "counts_0_9 = origins_top10['Count'].iloc[:9].values\n",
    "counts_1_10 = origins_top10['Count'].iloc[1:].values\n",
    "assert (counts_0_9 >= counts_1_10).all(), \\\n",
    "       \"Are your rows sorted in descending order?\"\n",
    "\n",
    "print(\"\\n(Passed! You got 30 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6c1f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 10** (`dests_test`: 20 points). The preceding code computed a tibble, `origins`, containing all the unique origins and their number of outgoing flights. Write some code to compute a new tibble, `dests`, which contains all unique destinations and their number of _incoming_ flights. Its columns should be named `DEST_AIRPORT_ID` (airport code) and `DEST_COUNT` (number of direct inbound segments).\n",
    "\n",
    "The test cell that follows prints the number of unique destinations and the first few rows of your result, as well as some automatic checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818856fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "print(\"Number of unique destinations:\", len(dests))\n",
    "dests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f143107",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `dests_test`\n",
    "\n",
    "if False:\n",
    "    dests.to_csv('dests_soln.csv', index=False)\n",
    "dests_soln = pd.read_csv('{}{}'.format(DATA_PATH, 'dests_soln.csv'))\n",
    "\n",
    "assert tibbles_are_equivalent(dests, dests_soln), \"Your solution does not match the instructors'.\"\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1315f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 11** (`dests_top10_test`: 20 points). Compute a tibble, `dests_top10`, containing the top 10 destinations (i.e., rows of `dests`) by inbound flight count. The column names should be the same as `origins_top10` and the rows should be sorted in decreasing order by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a603cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "print(\"Your computed top 10 destinations:\")\n",
    "dests_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0622f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `dests_top10_test`\n",
    "\n",
    "if False:\n",
    "    dests_top10.to_csv('dests_top10_soln.csv', index=False)\n",
    "dests_top10_soln = pd.read_csv('{}{}'.format(DATA_PATH, 'dests_top10_soln.csv'))\n",
    "\n",
    "print(\"=== Professor's solution ===\")\n",
    "display(dests_top10_soln)\n",
    "    \n",
    "assert tibbles_are_equivalent(dests_top10, dests_top10_soln), \\\n",
    "       \"Your table does not have the same entries as the solution.\"\n",
    "\n",
    "counts_0_9 = dests_top10['Count'].iloc[:9].values\n",
    "counts_1_10 = dests_top10['Count'].iloc[1:].values\n",
    "assert (counts_0_9 >= counts_1_10).all(), \\\n",
    "       \"Are your rows sorted in descending order?\"\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c6f36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The number of actual origins does equal the number of actual destinations. Let's store this value for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ce114",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "n_actual = len(set(origins['ORIGIN_AIRPORT_ID']) | set(dests['DEST_AIRPORT_ID']))\n",
    "print(\"Number of actual locations (whether origin or destination):\", n_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8855089",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Part 1: Constructing the state-transition matrix\n",
    "â€‹\n",
    "Now that you have cleaned up the data, let's prepare it for subsequent analysis. Start by constructing the _probability state-transition matrix_ for the airport network. Denote this matrix by $P \\equiv [p_{ij}]$, where $p_{ij}$ is the conditional probability that a random flyer departs from airport $i$ and arrives at airport $j$ given that he or she is currently at airport $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865791c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To build $P$, let's use SciPy's sparse matrix facilities. To do so, you will need to carry out the following two steps:\n",
    "\n",
    "1. _Map airport codes to matrix indices._ An `m`-by-`n` sparse matrix in SciPy uses the zero-based values 0, 1, ..., `m`-1 and 0, ..., `n`-1 to refer to row and column indices. Therefore, you will need to map the airport codes to such index values.\n",
    "2. _Derive weights, $p_{ij}$._ You will need to decide how to determine $p_{ij}$.\n",
    "\n",
    "Let's walk through each of these steps next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fb4ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step 1: Mapping airport codes to integers.** Luckily, you already have a code-to-integer mapping, which is in the column `airport_codes['Code']` mapped to the dataframe's `index`.\n",
    "\n",
    "As a first step, let's make note of the number of airports, which is nothing more than the largest index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb853ec7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "n_airports = airport_codes.index.max() + 1\n",
    "print(\"Note: There are\", n_airports, \"airports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108043d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, let's add another column to `segments` called `ORIGIN_INDEX`, which will hold the id corresponding to the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6b953",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Recall:\n",
    "segments.columns\n",
    "\n",
    "# Extract the `Code` column and index from `airport_codes`, storing them in\n",
    "# a temporary tibble with new names, `ORIGIN_AIRPORT_ID` and `ORIGIN_INDEX`.\n",
    "origin_indices = airport_codes[['Code']].rename(columns={'Code': 'ORIGIN_AIRPORT_ID'})\n",
    "origin_indices['ORIGIN_INDEX'] = airport_codes.index\n",
    "                               \n",
    "# Since you might run this code cell multiple times, the following\n",
    "# check prevents `ORIGIN_ID` from appearing more than once.\n",
    "if 'ORIGIN_INDEX' in segments.columns:\n",
    "    del segments['ORIGIN_INDEX']\n",
    "    \n",
    "# Perform the merge as a left-join of `segments` and `origin_ids`.\n",
    "segments = segments.merge(origin_indices, on='ORIGIN_AIRPORT_ID', how='left')\n",
    "segments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe031",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 12** (`dest_id_test`: 10 points). Analogous to the preceding procedure, create a new column called `segments['DEST_INDEX']` to hold the integer index of each segment's _destination_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f805510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "##BEGIN SOLUTION\n",
    "                               \n",
    "# Since you might run this code cell multiple times, the following\n",
    "# check prevents `ORIGIN_ID` from appearing more than once.\n",
    "\n",
    "    \n",
    "# Perform the merge as a left-join of `segments` and `origin_ids`.\n",
    "\n",
    "##END SOLUTION\n",
    "\n",
    "# Visually inspect your result:\n",
    "segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11315a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `dest_id_test`\n",
    "\n",
    "if False:\n",
    "    segments.to_csv('segments_soln.csv', index=False)\n",
    "segments_soln = pd.read_csv('{}{}'.format(DATA_PATH, 'segments_soln.csv'))\n",
    "\n",
    "assert tibbles_are_equivalent(segments, segments_soln), \\\n",
    "       \"Your solution does not match the instructors'.\"\n",
    "    \n",
    "print(\"\\n(Passed! You got 10 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a1d2d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step 2: Computing edge weights.** Armed with the preceding mapping, let's next determine each segment's transition probability, or \"weight,\" $p_{ij}$.\n",
    "\n",
    "For each origin $i$, let $d_i$ be the number of outgoing edges, or _outdegree_. Note that this value is *not* the same as the total number of (historical) outbound _segments_; rather, let's take $d_i$ to be just the number of airports reachable directly from $i$. For instance, consider all flights departing the airport whose airport code is 10135:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4459fc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(airport_codes[airport_codes['Code'] == 10135])\n",
    "\n",
    "abe_segments = segments[segments['ORIGIN_AIRPORT_ID'] == 10135]\n",
    "display(abe_segments)\n",
    "\n",
    "print(\"Total outgoing segments:\", abe_segments['FL_COUNT'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee9f1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "k_ABE = abe_segments['FL_COUNT'].sum()\n",
    "d_ABE = len(abe_segments)\n",
    "i_ABE = abe_segments['ORIGIN_AIRPORT_ID'].values[0]\n",
    "\n",
    "display(Markdown('''\n",
    "Though `ABE` has {} outgoing segments,\n",
    "its outdegree or number of outgoing edges is just {}.\n",
    "Thus, `ABE`, whose airport id is $i={}$, has $d_{{{}}} = {}$.\n",
    "'''.format(k_ABE, d_ABE, i_ABE, i_ABE, d_ABE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f737ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 13** (`weights_test`: 30 points). Add a new column named `OUTDEGREE` to the `segments` tibble that holds the outdegrees, $\\{d_i\\}$. That is, for each row whose airport _index_ (as opposed to code) is $i$, its entry of `OUTDEGREE` should be $d_i$.\n",
    "\n",
    "For instance, the rows of segments corresponding to airport ABE (code 10135 and matrix index 119) would look like this:\n",
    "\n",
    "ORIGIN_AIRPORT_ID | DEST_AIRPORT_ID | FL_COUNT | ORIGIN_INDEX | DEST_INDEX | OUTDEGREE\n",
    "------------------|-----------------|----------|--------------|------------|----------\n",
    "10135             | 10397           | 77       | 119          | 373        | 3\n",
    "10135             | 11433           | 85       | 119          | 1375       | 3\n",
    "10135             | 13930           | 18       | 119          | 3770       | 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This `if` removes an existing `OUTDEGREE` column\n",
    "# in case you run this cell more than once.\n",
    "if 'OUTDEGREE' in segments.columns:\n",
    "    del segments['OUTDEGREE']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Visually inspect the first ten rows of your result:\n",
    "segments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83de98",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `weights_test`\n",
    "\n",
    "if False:\n",
    "    segments.to_csv('segments_outdegree_soln.csv', index=False)\n",
    "    \n",
    "segments_outdegree_soln = pd.read_csv('{}{}'.format(DATA_PATH, 'segments_outdegree_soln.csv'))\n",
    "\n",
    "assert tibbles_are_equivalent(segments, segments_outdegree_soln), \\\n",
    "       \"Your solution does not appear to match the instructors'.\"\n",
    "\n",
    "print(\"\\n(Passed! You got 30 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbd097",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**From outdegree to weight.** Given the outdegree $d_i$, let $p_{ij} = \\frac{1}{d_i}$. In other words, suppose that a random flyer at airport $i$ is _equally likely_ to pick any of the destinations directly reachable from $i$. The following code cell stores that value in a new column, `WEIGHT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ad6c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if 'WEIGHT' in segments:\n",
    "    del segments['WEIGHT']\n",
    "    \n",
    "segments['WEIGHT'] = 1.0 / segments['OUTDEGREE']\n",
    "display(segments.head(10))\n",
    "\n",
    "# These should sum to 1.0!\n",
    "origin_groups = segments[['ORIGIN_INDEX', 'WEIGHT']].groupby('ORIGIN_INDEX')\n",
    "assert np.allclose(origin_groups.sum(), 1.0, atol=10*n_actual*np.finfo(float).eps), \"Rows of $P$ do not sum to 1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225e821",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 14** (`P_test`: 20 points). With your updated `segments` tibble, construct a sparse matrix, `P`, corresponding to the state-transition matrix $P$. Use SciPy's [scipy.sparse.coo_matrix()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to construct this matrix.\n",
    "\n",
    "The dimension of the matrix should be `n_airports` by `n_airports`. If an airport does not have any outgoing segments in the data, it should appear as a row of zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8479e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Visually inspect your result:\n",
    "spy(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3335642",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `P_test`\n",
    "\n",
    "assert type(P) is sp.sparse.coo.coo_matrix, \\\n",
    "       \"Matrix object has type {}, and is not a Numpy COO sparse matrix.\".format(type(P))\n",
    "assert P.shape == (n_airports, n_airports), \"Matrix has the wrong shape: it is {} x {} instead of {} x {}\".format(P.shape[0], P.shape[1], n_airports, n_airports)\n",
    "\n",
    "# Check row sums, which must be either 0 or 1\n",
    "n = P.shape[0]\n",
    "u = np.ones(n)\n",
    "row_sums = P.dot(u)\n",
    "is_near_zero = np.isclose(row_sums, 0.0, atol=10*n*np.finfo(float).eps)\n",
    "is_near_one = np.isclose(row_sums, 1.0, atol=10*n*np.finfo(float).eps)\n",
    "assert (is_near_zero | is_near_one).all()\n",
    "assert sum(is_near_one) == n_actual\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff56bbe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> **Note: Other formats.** The preceding code asked you to use coordinate (\"COO\") format to store the matrix. However, you may sometimes need to convert or use other formats. For example, SciPy provides many general graph processing algorithms in its [`csgraph` submodule](https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html). These routines expect the input graph as a sparse matrix, but one stored in compressed sparse row (\"CSR\") format rather than COO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d85e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Part 2, analysis: Computing the steady-state distribution\n",
    "\n",
    "Armed with the state-transition matrix $P$, you can now compute the steady-state distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c9a6a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 15** (`x0_test`: 20 points). At time $t=0$, suppose the random flyer is equally likely to be at any airport with an outbound segment, i.e., the flyer is at one of the \"actual\" origins. Create a NumPy vector `x0[:]` such that `x0[i]` equals this initial probability of being at airport `i`.\n",
    "\n",
    "> Note: If some airport $i$ has _no_ outbound flights, then be sure that $x_i(0) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your task: Create `x0` as directed above.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Visually inspect your result:\n",
    "def display_vec_sparsely(x, name='x'):\n",
    "    i_nz = np.argwhere(x).flatten()\n",
    "    df_x_nz = pd.DataFrame({'i': i_nz, '{}[i] (non-zero only)'.format(name): x[i_nz]})\n",
    "    display(df_x_nz.head())\n",
    "    print(\"...\")\n",
    "    display(df_x_nz.tail())\n",
    "    \n",
    "display_vec_sparsely(x0, name='x0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03da6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `x0_test`\n",
    "\n",
    "assert type(x0) is np.ndarray, \"`x0` does not appear to be a Numpy array.\"\n",
    "assert np.isclose(x0.sum(), 1.0, atol=10*n*np.finfo(float).eps), \"`x0` does not sum to 1.0, but it should.\"\n",
    "assert np.isclose(x0.max(), 1.0/n_actual, atol=10*n*np.finfo(float).eps), \"`x0` values seem off...\"\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0652868",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Exercise 16** (`eval_markov_chain_test`: 20 points). Given the state-transition matrix `P`, an initial vector `x0`, and the number of time steps `t_max`, complete the function `eval_markov_chain(P, x0, t_max)` so that it computes and returns $x(t_{\\textrm{max}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_markov_chain(P, x0, t_max):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return x\n",
    "\n",
    "T_MAX = 50\n",
    "x = eval_markov_chain(P, x0, T_MAX)\n",
    "display_vec_sparsely(x)\n",
    "\n",
    "print(\"\\n=== Top 10 airports ===\\n\")\n",
    "ranks = np.argsort(-x)\n",
    "top10 = pd.DataFrame({'Rank': np.arange(1, 11),\n",
    "                      'Code': airport_codes.iloc[ranks[:10]]['Code'],\n",
    "                      'Description': airport_codes.iloc[ranks[:10]]['Description'],\n",
    "                      'x(t)': x[ranks[:10]]})\n",
    "top10[['x(t)', 'Rank', 'Code', 'Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de9944",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `eval_markov_chain_test`\n",
    "\n",
    "print(x.sum())\n",
    "assert np.isclose(x.sum(), 1.0, atol=T_MAX*n_actual*np.finfo(float).eps)\n",
    "\n",
    "print(\"\\nTop 10 airports by Markov chain analysis:\\n\", list(top10['Code']))\n",
    "print(\"\\nCompare that to the Top 10 by (historical) outbound segments:\\n\", list(origins_top10['ID']))\n",
    "\n",
    "A = set(top10['Code'])\n",
    "B = set(origins_top10['ID'])\n",
    "C = (A - B) | (B - A)\n",
    "print(\"\\nAirports that appear in one list but not the other:\\n{}\".format(C))\n",
    "assert C == {11618, 11433, 12266, 14771, 14869, 12889, 14747, 12892}\n",
    "\n",
    "print(\"\\n(Passed! You got 20 points!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979a9de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Comparing the two rankings.** Before ending this notebook, let's create a table that compares our two rankings, side-by-side, where the first ranking is the result of the Markov chain analysis and the second from a ranking based solely on number of segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc6a60",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "top10_with_ranks = top10[['Code', 'Rank', 'Description']].copy()\n",
    "\n",
    "origins_top10_with_ranks = origins_top10[['ID', 'Description']].copy()\n",
    "origins_top10_with_ranks.rename(columns={'ID': 'Code'}, inplace=True)\n",
    "origins_top10_with_ranks['Rank'] = origins_top10.index + 1\n",
    "origins_top10_with_ranks = origins_top10_with_ranks[['Code', 'Rank', 'Description']]\n",
    "\n",
    "top10_compare = top10_with_ranks.merge(origins_top10_with_ranks, how='outer', on='Code',\n",
    "                                       suffixes=['_MC', '_Seg'])\n",
    "top10_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953ba6f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "How exciting! You've determined the top 10 airports at which a random flyer ends up, assuming he or she randomly selects directly reachable destinations. How does it compare, qualitatively, to a ranking based instead on (historical) outbound segments? Which ranking is a better measure of importance to the overall airport network?\n",
    "\n",
    "Think about what scenarios this could be useful. Potentially if you have a customer searching a website? The customer could start on any page,but you eventually would want to help drive them to check out. Knowing the likelihood each path hold, allows you to take actions to influence those paths, seeking your target outcome.\n",
    "\n",
    "Be sure to submit this notebook to get credit for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
