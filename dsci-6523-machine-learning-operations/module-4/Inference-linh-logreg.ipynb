{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f592439e-f375-4690-adc5-60365e2ca4e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is an auto-generated notebook to perform batch inference on a Spark DataFrame using a selected model from the model registry. This feature is in preview, and we would greatly appreciate any feedback through this form: https://databricks.sjc1.qualtrics.com/jfe/form/SV_1H6Ovx38zgCKAR0.\n",
    "\n",
    "## Instructions:\n",
    "1. Run the notebook against a cluster with Databricks ML Runtime version 15.2.x-cpu, to best re-create the training environment.\n",
    "2. Add additional data processing on your loaded table to match the model schema if necessary (see the \"Define input and output\" section below).\n",
    "3. \"Run All\" the notebook.\n",
    "4. Note: If the `%pip` does not work for your model (i.e. it does not have a `requirements.txt` file logged), modify to use `%conda` if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ccdb54-cac6-49ea-a0da-4db710c73401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"telco_linh_logreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b919f14-c6d9-4c8e-af3d-6c8208abdecf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Environment Recreation\n",
    "Run the notebook against a cluster with Databricks ML Runtime version 15.2.x-cpu, to best re-create the training environment.. The cell below downloads the model artifacts associated with your model in the remote registry, which include `conda.yaml` and `requirements.txt` files. In this notebook, `pip` is used to reinstall dependencies by default.\n",
    "\n",
    "### (Optional) Conda Instructions\n",
    "Models logged with an MLflow client version earlier than 1.18.0 do not have a `requirements.txt` file. If you are using a Databricks ML runtime (versions 7.4-8.x), you can replace the `pip install` command below with the following lines to recreate your environment using `%conda` instead of `%pip`.\n",
    "```\n",
    "conda_yml = os.path.join(local_path, \"conda.yaml\")\n",
    "%conda env update -f $conda_yml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24e31a97-16e1-451b-98fb-d6e87e0067d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "import os\n",
    "\n",
    "model_uri = f\"models:/{model_name}/1\"\n",
    "local_path = ModelsArtifactRepository(model_uri).download_artifacts(\"\") # download model from remote registry\n",
    "\n",
    "requirements_path = os.path.join(local_path, \"requirements.txt\")\n",
    "if not os.path.exists(requirements_path):\n",
    "  dbutils.fs.put(\"file:\" + requirements_path, \"\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae730a73-e8e9-41ac-a599-bde97cf3b8f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $requirements_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69973050-4254-4e4c-97ae-43e8ce8005c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define input and output\n",
    "The table path assigned to`input_table_name` will be used for batch inference and the predictions will be saved to `output_table_path`. After the table has been loaded, you can perform additional data processing, such as renaming or removing columns, to ensure the model and table schema matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f59740f7-7f7d-410c-bd87-cc34b1e73fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# redefining key variables here because %pip and %conda restarts the Python interpreter\n",
    "model_name = \"telco_linh_logreg\"\n",
    "input_table_name = \"mlops.telco_customer_churn\"\n",
    "output_table_path = \"/FileStore/batch-inference/telco_linh_logreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3adc7e-2c9b-4869-9e60-4373d6a8366f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load table as a Spark DataFrame\n",
    "table = spark.table(input_table_name)\n",
    "\n",
    "# optionally, perform additional data processing (may be necessary to conform the schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442e71de-8711-4ef5-abb2-c5a1fcc602c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load model and run inference\n",
    "**Note**: If the model does not return double values, override `result_type` to the desired type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c728bb51-8e6f-45dd-9cea-2562fb5f9839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "model_uri = f\"models:/{model_name}/1\"\n",
    "\n",
    "# create spark user-defined function for model prediction\n",
    "predict = mlflow.pyfunc.spark_udf(spark, model_uri, result_type=\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b350e1c7-b059-4e4d-bba6-2430e47d258b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df = table.withColumn(\"prediction\", predict(struct(*table.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef735d9b-ca14-4870-a159-703e0c587268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save predictions\n",
    "**The default output path on DBFS is accessible to everyone in this Workspace. If you want to limit access to the output you must change the path to a protected location.**\n",
    "The cell below will save the output table to the specified FileStore path. `datetime.now()` is appended to the path to prevent overwriting the table in the event that this notebook is run in a batch inference job. To overwrite existing tables at the path, replace the cell below with:\n",
    "```python\n",
    "output_df.write.mode(\"overwrite\").save(output_table_path)\n",
    "```\n",
    "\n",
    "### (Optional) Write predictions to Unity Catalog\n",
    "If you have access to any UC catalogs, you can also save predictions to UC by specifying a table in the format `<catalog>.<database>.<table>`.\n",
    "```python\n",
    "output_table = \"\" # Example: \"ml.batch-inference.telco_linh_logreg\"\n",
    "output_df.write.saveAsTable(output_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ca58e9-bad5-4d18-b3f3-18419cb6d648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# To write to a unity catalog table, see instructions above\n",
    "output_df.write.save(f\"{output_table_path}_{datetime.now().isoformat()}\".replace(\":\", \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6041c367-4ca4-410c-9180-90c1bd3fa457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Inference-linh-logreg",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
